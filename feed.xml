<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="cn"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://peng00bo00.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://peng00bo00.github.io/" rel="alternate" type="text/html" hreflang="cn"/><updated>2024-07-19T15:13:43+00:00</updated><id>https://peng00bo00.github.io/feed.xml</id><title type="html">Bo’s Blog</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">GAMES001课程笔记14-深度学习</title><link href="https://peng00bo00.github.io/blog/2024/GAMES001-NOTES-14/" rel="alternate" type="text/html" title="GAMES001课程笔记14-深度学习"/><published>2024-07-04T00:00:00+00:00</published><updated>2024-07-04T00:00:00+00:00</updated><id>https://peng00bo00.github.io/blog/2024/GAMES001-NOTES-14</id><content type="html" xml:base="https://peng00bo00.github.io/blog/2024/GAMES001-NOTES-14/"><![CDATA[<blockquote class="block-preface"> <p>这个系列是GAMES001-图形学中的数学(<a href="https://games-cn.org/games001/">GAMES 001: Mathematics in Computer Graphics</a>)的同步课程笔记。课程旨在总结归纳图形学学习过程中重要的数学概念、理论和方法，并将以索引的形式在每一章节中穿插讲解该数学专题在图形学中的应用。本课程既可以作为GAMES系列其它课程学习的基础或”手册”，也可以作为站在不一样的视角复习图形学知识的平台。本节主要介绍求解深度学习中生成式模型相关的数学知识。</p> </blockquote> <h2 id="深度学习">深度学习</h2> <p>深度学习作为当前最热门的话题之一，已经渗透到计算机科学的各个研究领域中。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/JWUry2U.png" width="100%"/> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/MWJEAik.png" width="100%"/> </div> <p>从数学知识的角度来看，深度学习的基础无非是概率论、线性代数以及数学优化。当然，从学习这些深度学习相关的数学基础到掌握当前最前沿的深度学习模型之间，仍然有非常大的距离。本节课的目标是从当前热门的生成式模型入手，介绍相关的数学知识。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/dt2eppu.png" width="100%"/> </div> <h3 id="损失函数">损失函数</h3> <p>从<a href="/2024/04/10/GAMES001-NOTES-05.html">拟合</a>的角度来看，深度学习的目标是从一系列已知的数据点中学习到一个能够很好拟合这些数据的模型。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/XBkb2XV.png" width="100%"/> </div> <p>我们可以暂时忽略神经网络架构的各种细节，把神经网络视为一个关于参数\(\theta\)的函数\(f_{\theta} (x)\)。这样，模型训练的过程实际上就是最小化关于\(\theta\)的损失函数的过程，通常通过随机梯度下降法来实现。根据不同类型的问题，可以选择相应的损失函数来进行处理。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/Z7FX19O.png" width="100%"/> </div> <h3 id="生成式模型">生成式模型</h3> <p>目前，生成式模型是整个AI领域中最为热门的研究方向。从概率的角度来看，每个数据样本都来自于某个概率分布。只要我们能够通过神经网络近似这个分布，就可以通过采样的方式生成新样本。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/24kn3mc.png" width="100%"/> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/xQ6diI9.png" width="100%"/> </div> <p>然而，生成式模型需要回答以下两个问题：</p> <ol> <li>如何度量真实数据分布\(p(x)\)与网络\(\pi_\theta (x)\)之间的相似程度？</li> <li>如何使用神经网络来表示高维数据的分布？</li> </ol> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/KyOsMaO.png" width="100%"/> </div> <p>对于第一个问题，通常可以使用<a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">KL散度</a>来进行处理，它度量了\(p\)和\(q\)两个概率分布之间的差异。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/67Njibg.png" width="100%"/> </div> <p>假设\(p\)是一个已知的概率分布，则最小化\(p\)和\(q\)之间KL散度等价于对\(q\)进行最大似然估计。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/484kltM.png" width="100%"/> </div> <h2 id="vae">VAE</h2> <h3 id="隐变量">隐变量</h3> <p>KL散度可以用来解决度量概率分布之间相似度的问题。在此基础上，我们可以利用<a href="https://en.wikipedia.org/wiki/Variational_autoencoder">变分自编码器(VAE)</a>来对概率分布进行建模。VAE是生成式模型中的经典方法，它的核心在于构造一个服从标准正态分布的隐变量\(z \sim N(0, I)\)，且其维度要远小于真实数据\(x\)的维度。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/GT0PBdd.png" width="100%"/> </div> <p>每个数据样本都对应着一个隐变量分布，它们之间的关系可以通过联合概率进行描述。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/Xq8gzgM.png" width="100%"/> </div> <p>在VAE模型中，数据分布\(p(x)\)和隐变量分布\(p(z)\)通过<strong>编码器(encoder)</strong>和<strong>解码器(decoder)</strong>关联在一起：</p> <ul> <li>编码器将数据\(x\)映射为隐变量\(z\)，相当于计算条件概率\(p(z \vert x)\)。</li> <li>解码器将隐变量\(z\)重建为数据\(x\)，相当于计算条件概率\(p(x \vert z)\)。</li> </ul> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/os5LrLe.png" width="100%"/> </div> <p>有了条件概率后就可以计算样本数据\(x\)在VAE网络中的概率</p> \[\pi_\theta (x) = \int p_\theta (x \vert z) p (z) \ \mathrm{d} z = \mathbb{E}_{z \sim p(z)} [p_\theta (x \vert z)]\] <p>进而通过最小化KL散度来进行训练。然而，需要注意的是，在计算期望\(\mathbb{E}_{z \sim p(z)} [p_\theta (x \vert z)]\)时需要对\(z\)进行采样，通常需要非常多的样本才能保证计算结果的准确性。除此之外，我们还希望数据\(x\)和隐变量\(z\)之间保持有相对有序的对应关系。因此，需要借助编码器来规范\(z\)的行为。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/09wvIqE.png" width="100%"/> </div> <p>具体来说，在VAE中使用两个神经网络来表示条件概率：</p> <ul> <li>解码器\(D_\theta (z)\)输出\(p_\theta (x \vert z)\)的均值，而\(p_\theta (x \vert z)\)的方差一般规定为1。</li> <li>编码器输出\(q_\phi (z \vert x)\)的均值和方差，这里假定了\(q_\phi (z \vert x)\)服从正太分布</li> </ul> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/ZBtODYG.png" width="100%"/> </div> <p>这样，联合概率分布\(p(x, z)\)就有两种表达方式：</p> <ul> <li>对隐变量\(z\)进行采样并利用解码器有\(p(x, z) = p_\theta (x \vert z) p(z)\)</li> <li>对数据\(x\)进行采样并利用编码器有\(q(x, z) = q_\phi (z \vert x) p (x)\)</li> </ul> <p>两种表示方法对应着同一个分布，因此可以使用KL散度\(D_{KL} (q(x, z) \Vert p(x, z))\)作为损失函数进行训练。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/FBYt3pv.png" width="100%"/> </div> <h3 id="损失函数-1">损失函数</h3> <p>把KL散度进行展开并略去只关于样本分布\(p(x)\)的部分可以得到最终的损失函数为</p> \[L = \mathbb{E}_{z \sim q_\phi (z \vert x_i)} \bigg[ \log{\frac{q_\phi (z \vert x_i)}{p(x_i, z)}} \bigg]\] <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/jq25Sqd.png" width="100%"/> </div> <p>实际上，上面定义的损失函数还对应着\(\log p(x)\)的下界，也即最大似然估计。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/Z1Sx5vG.png" width="100%"/> </div> <p>继续对损失函数进行展开，可以将损失函数拆分为两部分：</p> \[L = \mathbb{E}_{z \sim q_\phi (z \vert x_i)} [-\log{p_\theta (x_i \vert z)}] + D_{KL} (q_\phi (z \vert x_i) \Vert p(z))\] <p>其中第一项对应着生成结果与原始数据之间的误差(重建误差)，而第二项对应着两个正态分布之间的KL散度。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/lhspieW.png" width="100%"/> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/3rfe61k.png" width="100%"/> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/hmlNpWm.png" width="100%"/> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/cLJ6Put.png" width="100%"/> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/VRwYr0M.png" width="100%"/> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/I2m6Grt.png" width="100%"/> </div> <h3 id="训练与生成">训练与生成</h3> <p>推导完VAE损失函数后就可以按照通常的神经网络来进行训练。而需要进行生成时只需要从标准正态分布进行采样，并送入解码器来得到新样本。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/lU1FNvf.png" width="100%"/> </div> <p>VAE模型的一个缺陷在于它很难生成高质量的数据。对于图像生成任务，这表现为生成的图片大多比较”糊”。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/B3ObRkt.jpg" width="100%"/> </div> <h2 id="diffusion-model">Diffusion Model</h2> <p>目前火热的<strong>扩散模型(diffusion model)</strong>是最新一代的生成式模型。与VAE相比，扩散模型能够生成更高清、更高质量的图像。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/ptH05lR.jpg" width="100%"/> </div> <p>从数学上来说，扩散模型和VAE之间有许多相通之处。根据上一节的推导，VAE的核心在于建立数据分布和隐变量空间之间的双向映射关系。扩散模型也有类似的思想，不过在扩散模型中，随机噪声分布和数据分布是通过一系列双向映射来实现的。通过这样多步的映射，扩散模型具有更强的表达能力，生成的数据质量也更高。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/IaG3yxU.png" width="100%"/> </div> <h3 id="前向过程">前向过程</h3> <p>扩散模型包括前向和反向两个过程，其中前向过程表示对一张给定的图片添加噪声直至它完全变成无法用肉眼识别的噪声图像。这里每一步添加噪声可以表示为一个马尔科夫链，即第\(t\)步的结果只与上一步\(t-1\)的状态有关。假设在每一步中图像都服从正态分布，则添加噪声的过程可以表示为</p> \[q(x_t \vert x_{t-1}) = N(\sqrt{1 - \beta_t} x_{t-1}, \beta_t I)\] <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/WFghg2M.png" width="100%"/> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/ZzsdESr.png" width="100%"/> </div> <p>利用重参数化的技巧，添加噪声的过程可以表示为</p> \[x_t = \sqrt{\alpha_t} x_{t-1} + \sqrt{1 - \alpha_t} \varepsilon_{t-1}\] <p>其中\(\varepsilon\)表示来自于标准正态分布的随机噪声。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/BcrDPPV.png" width="100%"/> </div> <p>再结合正态分布的性质，我们对\(x_t\)的迭代公式进行展开，最终得到\(x_t\)与初始图像\(x_0\)之间的关系式</p> \[x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \varepsilon_t\] \[\bar{\alpha}_t = \alpha_t \alpha_{t-1} \cdots \alpha_1\] <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/HpfYlfM.png" width="100%"/> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/1oPLF6W.png" width="100%"/> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/xjNRAqL.png" width="100%"/> </div> <p>总结一下，扩散模型的前向过程是一个马尔科夫链，也可以通过一次采样从\(x_0\)直接得到\(x_t\)。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/XTeuLkC.png" width="100%"/> </div> <h3 id="反向过程">反向过程</h3> <p>扩散模型的反向过程是从噪声\(x_t\)中逐步恢复\(x_0\)的过程，其中的每一步都可以使用神经网络来表示反向概率\(p_\theta (x_{t-1} \vert x_t)\)。类似于VAE，我们同样假设\(p_\theta (x_{t-1} \vert x_t)\)是正态分布，其均值由网络给出。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/XBEr4To.png" width="100%"/> </div> <h3 id="损失函数-2">损失函数</h3> <p>扩散模型的损失函数与VAE同样是类似的，不过这里我们需要把单个隐变量\(z\)替换为一系列变量\(x_t\)，…，\(x_1\)。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/rwei8VQ.png" width="100%"/> </div> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/eod4vua.png" width="100%"/> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/TEr0UaC.png" width="100%"/> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/eN3CQVp.png" width="100%"/> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/Ne7VuGZ.png" width="100%"/> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/Yab3Psz.png" width="100%"/> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/KWSYLPg.png" width="100%"/> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/F3g7N8w.png" width="100%"/> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/t9b0Hpt.png" width="100%"/> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/RvP9Ws5.png" width="100%"/> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/M0P3C0b.png" width="100%"/> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/4Bzwrhk.png" width="100%"/> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/iwG0eED.png" width="100%"/> </div> <h2 id="reference">Reference</h2> <ul> <li><a href="https://www.bilibili.com/video/BV1MF4m1V7e3?p=14&amp;vd_source=7a2542c6c909b3ee1fab551277360826">Lecture 15: 生成模型</a></li> </ul>]]></content><author><name></name></author><category term="GAMES001"/><category term="CG"/><category term="Math"/><summary type="html"><![CDATA[生成式模型背后的数学知识]]></summary></entry><entry><title type="html">GAMES001课程笔记13-优化基础</title><link href="https://peng00bo00.github.io/blog/2024/GAMES001-NOTES-13/" rel="alternate" type="text/html" title="GAMES001课程笔记13-优化基础"/><published>2024-07-03T00:00:00+00:00</published><updated>2024-07-03T00:00:00+00:00</updated><id>https://peng00bo00.github.io/blog/2024/GAMES001-NOTES-13</id><content type="html" xml:base="https://peng00bo00.github.io/blog/2024/GAMES001-NOTES-13/"><![CDATA[<blockquote class="block-preface"> <p>这个系列是GAMES001-图形学中的数学(<a href="https://games-cn.org/games001/">GAMES 001: Mathematics in Computer Graphics</a>)的同步课程笔记。课程旨在总结归纳图形学学习过程中重要的数学概念、理论和方法，并将以索引的形式在每一章节中穿插讲解该数学专题在图形学中的应用。本课程既可以作为GAMES系列其它课程学习的基础或”手册”，也可以作为站在不一样的视角复习图形学知识的平台。本节主要介绍求解数学优化的相关技术。</p> </blockquote> <p>数学优化是一个非常广泛且重要的研究领域。从形式上来看，优化问题的目标是最小化一个函数\(f(x)\)。在这个过程中，优化目标函数\(f\)和待优化的变量\(x\)既可以是离散的，也可以是连续的。同时，变量\(x\)还可能需要满足一定的约束条件\(C\)，记为\(x \in C\)。针对不同类型的优化问题，人们设计了各种优化算法来进行求解。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/WTs2m5l.png" width="100%"/> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/LMxJqSf.png" width="100%"/> </div> <h2 id="无约束优化">无约束优化</h2> <p>优化问题中最基础的是无约束优化问题。此时可以令\(C\)为全空间，同时假设\(x \in \mathbb{R}^n\)是连续的向量，且目标函数\(f(x)\)是连续可微的函数。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/OQvhH9f.png" width="100%"/> </div> <p>在这样的框架下，许多问题，例如最小二乘法、曲面参数化以及弹性体仿真等，都可以等价地表示为无约束优化问题。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/FAA7Ty4.png" width="100%"/> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/vlkrNxe.png" width="100%"/> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/73enMta.png" width="100%"/> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/jyDLEJm.png" width="100%"/> </div> <h3 id="线搜索">线搜索</h3> <p><strong>线搜索(line search)</strong>是求解无约束优化的一类经典方法，它的基本思想是从当前位置\(x_k\)出发向\(p_k\)方向移动\(\alpha_k\)步长来逐步最小化目标函数。显然，线搜索方法的核心在于如何确定移动方向\(p_k\)以及步长\(\alpha_k\)这两个量。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/gW9m06U.png" width="100%"/> </div> <h4 id="梯度下降">梯度下降</h4> <p><strong>梯度下降(gradient descent)</strong>是线搜索中最常见的一种方法。在梯度下降算法中，我们选择目标函数的负梯度方向作为前进方向，即\(p_k = -\nabla f (x_k)\)。而要确定移动步长\(\alpha_k\)则相对复杂一些，实践中常使用Armijo条件来进行计算。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/4JAd48J.png" width="100%"/> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/GgE5uGP.png" width="100%"/> </div> <p>结合Armijo条件的梯度下降算法流程可以参考下面ppt。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/KFrlfFW.png" width="100%"/> </div> <h4 id="牛顿迭代">牛顿迭代</h4> <p>梯度下降法只考虑了优化目标函数的一阶信息。为了获得更快的收敛速度，可以结合目标函数的二阶信息，此时得到的算法称为牛顿法。使用牛顿法时，通常要求目标函数在当前位置的Hessian矩阵为正定矩阵。如果Hessian矩阵不是正定的，则需要使用一些额外的方法将其转换为正定矩阵来进行处理。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/DJn7sdz.png" width="100%"/> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/fr0dSrk.png" width="100%"/> </div> <p>牛顿法的算法流程可以参考下面的ppt。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/z82G7YR.png" width="100%"/> </div> <p>与梯度下降法相比，牛顿法通常具有更快的收敛速度，并且可以避免一些震荡的情况。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/0nD7vSJ.png" width="100%"/> </div> <h4 id="拟牛顿法">拟牛顿法</h4> <p>牛顿法的一个缺陷在于每一步迭代中需要计算Hessian矩阵并求解线性方程组来获得前进方向。在某些问题中，这两个步骤可能比较困难且相对耗时。针对这样的问题，我们可以使用一个更容易计算的矩阵\(B_k\)来近似Hessian矩阵，这样的一类方法称为拟牛顿法。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/275ibgv.png" width="100%"/> </div> <p><a href="https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm#:~:text=In%20numerical%20optimization%2C%20the%20Broyden,the%20gradient%20with%20curvature%20information.">BFGS算法</a>是目前应用最为广泛的一种拟牛顿法。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/DBepGGo.png" width="100%"/> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/W2EIa4b.png" width="100%"/> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/xc3IEAm.png" width="100%"/> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/81RR0W0.png" width="100%"/> </div> <p>BFGS算法的流程可以参考下面的ppt。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/JsaYSWP.png" width="100%"/> </div> <p>在BFGS算法的基础上，进一步发展出了<a href="https://en.wikipedia.org/wiki/Limited-memory_BFGS">L-BFGS算法</a>，旨在降低算法在计算和存储方面的开销。目前，L-BFGS是许多数值计算软件中默认的优化算法。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/JoaB4rD.png" width="100%"/> </div> <h3 id="不动点迭代">不动点迭代</h3> <p>从优化的角度来看，我们之前介绍过的<a href="/blog/2024/GAMES001-NOTES-12#不动点迭代">不动点迭代</a>实际上也是一种拟牛顿法。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/TNSZY3J.png" width="100%"/> </div> <h2 id="带约束优化">带约束优化</h2> <p>对于带约束的优化问题，约束可以分为等式约束以及不等式约束两类。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/kApbxYK.png" width="100%"/> </div> <p>在物理仿真中的常见约束都可以转换为等式约束以及不等式约束。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/COXgyoh.png" width="100%"/> </div> <h3 id="等式约束">等式约束</h3> <p>对于等式约束的情况，我们首先需要考虑在什么情况下目标函数能够达到最优。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/B6iWDDi.png" width="100%"/> </div> <p>以二维情况为例，只有在目标函数的梯度\(\nabla f(x, y)\)与约束\(g(x, y)\)的梯度相互平行时能够达到最优，即\(\nabla f = \lambda \nabla g\)。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/5buopvh.png" width="100%"/> </div> <p>根据上面的分析可以得到等式约束条件下最优点\(x^*\)需要满足的条件：</p> \[g_i(x^*) = 0\] \[\nabla f(x^*) = \sum_i \lambda_i \nabla g_i (x^*)\] <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/n5bsPpo.png" width="100%"/> </div> <h4 id="拉格朗日乘子法">拉格朗日乘子法</h4> <p>更进一步，等式约束条件的最优化问题可以利用<a href="https://en.wikipedia.org/wiki/Lagrange_multiplier">拉格朗日乘子</a>进行处理。我们定义原始优化问题的拉格朗日函数为</p> \[L(x, \lambda_1, ..., \lambda_n) = f(x) - \sum_i \lambda_i g_i (x)\] <p>则等式约束的最优解可以表示为</p> \[\nabla L = 0\] <p>这样约束优化问题就通过拉格朗日乘子转换为了无约束优化问题。然而，需要注意的是仅通过最小化\(L\)并不能保证得到问题的解，还必须确保\(g_i (x) = 0\)始终成立。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/0BIa5C9.png" width="100%"/> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/8iv1JeF.png" width="100%"/> </div> <h4 id="min-max问题">Min-Max问题</h4> <p>实际上，与原始约束优化问题等价的拉格朗日优化问题是一个Min-Max问题：</p> \[\min_x \max_{\lambda_i} L(x, \lambda_1, ..., \lambda_n)\] <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/TJwiUyJ.png" width="100%"/> </div> <p>对于Min-Max问题，我们无法直接使用前面介绍过的无约束优化方法进行求解。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/PdkIdqi.png" width="100%"/> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/lkjae3m.png" width="100%"/> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/bJSJ9QX.png" width="100%"/> </div> <h4 id="对偶问题">对偶问题</h4> <p>直接求解Min-Max问题往往效率较低。在实践中，通常会将Min-Max问题转换为其对偶问题进行处理。这时需要交换最大化和最小化的顺序，先对\(x\)进行最小化，然后对\(\lambda\)进行最大化，从而将Min-Max问题转化为Max-Min问题。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/SkVlcRH.png" width="100%"/> </div> <p>对于一般的函数\(L(x, \lambda)\)，可以证明对偶问题是原始问题的下界，这称为弱对偶关系。而对于凸函数，可以证明原始问题和对偶问题是等价的，称为强对偶条件。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/QuC5bIq.png" width="100%"/> </div> <p>假设强对偶条件成立，则可以使用无约束优化问题的相关求解方法进行处理。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/Hm9Jswk.png" width="100%"/> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/jr8zxod.png" width="100%"/> </div> <h4 id="对偶上升法">对偶上升法</h4> <p>对于对偶问题，我们可以使用对偶上升法交替更新\(x\)和\(\lambda\)进行求解。对偶上升法将原始的约束优化问题转换为了两个无约束优化问题，在优化\(\lambda\)时可以利用梯度下降法进行处理。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/EelKXj3.png" width="100%"/> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/JFo3pwr.png" width="100%"/> </div> <h4 id="罚函数法">罚函数法</h4> <p>除了拉格朗日乘子法之外，对于等式约束的优化问题还可以使用罚函数法。其基本思想是将等式约束视为一个惩罚项，当\(x\)违反约束时施加相应的惩罚来约束\(x\)。然而，罚函数法无法保证约束一定能够严格满足，只能保证近似满足。实践中一般需要调节惩罚项系数\(\mu\)来保证优化能够收敛。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/aBswLUb.png" width="100%"/> </div> <h3 id="不等式约束">不等式约束</h3> <p>带有不等式约束的优化问题处理起来比等式约束更加复杂。关键在于，不等式约束只有在变成等式约束时才会直接影响最优解；否则，不等式约束相当于不起作用，此时可以按照无约束优化进行处理。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/yNiBb1t.png" width="100%"/> </div> <p>对于一般形式的不等式约束优化问题，我们同样可以定义拉格朗日函数并推导相应的对偶问题以及对偶优化算法。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/Izia8C3.png" width="100%"/> </div> <h2 id="reference">Reference</h2> <ul> <li><a href="https://www.bilibili.com/video/BV1MF4m1V7e3?p=15&amp;vd_source=7a2542c6c909b3ee1fab551277360826">Lecture 14: 优化基础</a></li> </ul>]]></content><author><name></name></author><category term="GAMES001"/><category term="CG"/><category term="Math"/><summary type="html"><![CDATA[图形学中常用的优化方法]]></summary></entry><entry><title type="html">GAMES001课程笔记12-线性系统</title><link href="https://peng00bo00.github.io/blog/2024/GAMES001-NOTES-12/" rel="alternate" type="text/html" title="GAMES001课程笔记12-线性系统"/><published>2024-06-21T00:00:00+00:00</published><updated>2024-06-21T00:00:00+00:00</updated><id>https://peng00bo00.github.io/blog/2024/GAMES001-NOTES-12</id><content type="html" xml:base="https://peng00bo00.github.io/blog/2024/GAMES001-NOTES-12/"><![CDATA[<blockquote class="block-preface"> <p>这个系列是GAMES001-图形学中的数学(<a href="https://games-cn.org/games001/">GAMES 001: Mathematics in Computer Graphics</a>)的同步课程笔记。课程旨在总结归纳图形学学习过程中重要的数学概念、理论和方法，并将以索引的形式在每一章节中穿插讲解该数学专题在图形学中的应用。本课程既可以作为GAMES系列其它课程学习的基础或”手册”，也可以作为站在不一样的视角复习图形学知识的平台。本节主要介绍求解线性系统的相关技术。</p> </blockquote> <p>求解线性系统实际上就是解矩阵方程</p> \[\mathbf{A} \boldsymbol{x} = \mathbf{b}\] <p>在图形学的各个研究方向中，许多问题的本质就是求解一个线性系统。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/z9uR9SM.png" width="100%"/> </div> <h2 id="直接求解">直接求解</h2> <h3 id="高斯消元">高斯消元</h3> <p>求解线性系统的基本方法是直接进行求解，其中<a href="https://en.wikipedia.org/wiki/Gaussian_elimination">高斯消元法</a>是这类方法的典型代表。具体来说，高斯消元法会同时对系数矩阵\(\mathbb{A}\)和向量\(\mathbf{b}\)进行行变换，使得矩阵\(\mathbb{A}\)变成一个上三角矩阵。得到上三角矩阵后即可从下往上反向回代计算出解\(\boldsymbol{x}\)。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/oOZZ6q2.png" width="100%"/> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/4CHiPgb.png" width="100%"/> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/1CVjWRz.png" width="100%"/> </div> <p>换个角度来看，矩阵的行变换等价于左乘一个下三角矩阵。因此，高斯消元法可以理解为对矩阵进行<a href="https://en.wikipedia.org/wiki/LU_decomposition">LU分解</a>。通过LU分解，我们将系数矩阵\(\mathbf{A}\)分解为一个下三角矩阵\(\mathbf{L}\)和一个上三角矩阵\(\mathbf{U}\)的乘积。然后，通过求解两个较简单的线性系统\(\mathbf{L} \mathbf{y} = \mathbf{b}\)和\(\mathbf{U} \boldsymbol{x} = \mathbf{y}\)就可以得到最终解\(\boldsymbol{x}\)。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/MDCebWs.png" width="100%"/> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/PWnsVgm.png" width="100%"/> </div> <p>从编程的角度来讲，高斯消元法的核心在于计算矩阵的LU分解，其伪代码可以参考下方ppt。显然LU分解的复杂度为\(O(n^3)\)，因此LU分解是一个复杂度比较高的算法，不过可以利用一些并行的方法来进行加速。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/lUdfUoQ.png" width="100%"/> </div> <p>使用高斯消元法时还需要注意，如果当前选择的对角线元素值接近0时会产生数值计算上的不稳定。为了处理这种情况，可以考虑每次都选择当前列中绝对值最大的那一行作为主元，然后再利用主元来消去其它行。这种带选择主元的高斯消元法也称为列主元高斯消元法或部分主元高斯消元法(Gaussian Elimination with Partial Pivoting, GEPP)。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/BPOKxPg.png" width="100%"/> </div> <h3 id="cholesky分解">Cholesky分解</h3> <p>对于对称半正定的系数矩阵，LU分解可以简化为<a href="https://en.wikipedia.org/wiki/Cholesky_decomposition">Cholesky分解</a>。此时，系数矩阵\(\mathbf{A}\)可以表示为下三角矩阵\(\mathbf{A}\)与其自身转置\(\mathbf{A}^T\)的乘积。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/Fo6uU0D.png" width="100%"/> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/OPsneo2.png" width="100%"/> </div> <p>Cholesky分解的伪代码可以参考下方ppt。类似于LU分解，Cholesky分解的复杂度同样是\(O(n^3)\)，也可以利用一些并行的技术来进行加速。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/dciPYSX.png" width="100%"/> </div> <p>简单总结一下，LU分解以及高斯消元法可以求解任意的矩阵方程，而Cholesky分解则只适用于对称半正定的系统。由于这两种方法都具有\(O(n^3)\)的复杂度，它们只适合一些规模比较小的线性系统。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/ARqmPdk.png" width="100%"/> </div> <h2 id="迭代求解">迭代求解</h2> <p>对于更大规模的线性系统，我们通常会使用迭代法来求解。迭代法的基本框架是从一个初始解\(x^0\)出发，反复执行某个步骤来更新当前解</p> \[x^{k+1} \leftarrow \Psi (x^k)\] <p>迭代过程会继续直到满足设定的迭代次数上限，或者达到一定的残差收敛条件。相较于直接法，迭代法的优势在于可以在任意迭代步骤中得到精度满足要求的近似解，而不需要等到整个算法结束才能终止。在许多应用场景中，仅需一个近似解即可满足需求。同时，一个好的初始解\(x^0\)往往可以极大地减少迭代次数。因此对于特定问题优化的迭代算法有着远高于直接求解的效率。除此之外，迭代法甚至可以推广到系数矩阵\(\mathbf{A}\)没有显式给出的情况。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/SjBJZO3.png" width="100%"/> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/YS8P1us.png" width="100%"/> </div> <h3 id="不动点迭代">不动点迭代</h3> <p>首先我们来介绍<a href="https://en.wikipedia.org/wiki/Fixed-point_iteration">不动点迭代</a>。举个简单的例子，假设要求解方程</p> \[x = \cos{x}\] <p>我们可以建立迭代格式</p> \[x^{k+1} \leftarrow \cos{x^k}\] <p>通过不断执行可以发现\(\boldsymbol{x}\)会收敛到某个值上，而实际上这个值就是方程的解。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/fjXgdF9.png" width="100%"/> </div> <p>在线性系统的求解中，可以利用不动点迭代方法来进行计算。我们可以将系数矩阵\(\mathbf{A}\)表示为</p> \[\mathbf{A} = \mathbf{M} - \mathbf{N}\] <p>这样线性系统的解可以表示为</p> \[\boldsymbol{x} = \mathbf{M}^{-1} (\mathbf{N} \boldsymbol{x} + \mathbf{b})\] <p>从而建立迭代格式</p> \[\boldsymbol{x}^{k+1} \leftarrow \mathbf{M}^{-1} (\mathbf{N} \boldsymbol{x}^k + \mathbf{b})\] <p>如果迭代能够收敛，则可以保证\(\boldsymbol{x}^k\)会趋向于线性系统的解。以上就是不动点迭代求解线性系统的基本思想，当然使用不动点迭代时还有很多问题需要考虑，比如如何计算逆阵\(\mathbf{M}^{-1}\)、如何保证迭代收敛以及迭代的效率等。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/pEcNTUk.png" width="100%"/> </div> <h4 id="jacobi迭代">Jacobi迭代</h4> <p><a href="https://en.wikipedia.org/wiki/Jacobi_method">Jacobi迭代</a>是一种典型的不动点迭代法。在Jacobi迭代中，我们取\(\mathbf{M}\)矩阵为系数矩阵\(\mathbf{A}\)的对角元素\(\mathbf{D}\)，则\(\mathbf{N}\)矩阵为系数矩阵\(\mathbf{A}\)的其余非对角元素的相反数。此时\(\mathbf{M}^{-1}\)为对角元素的倒数，因此每次迭代都只需要进行基本的矩阵乘法和加法运算。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/RiGG3Vf.png" width="100%"/> </div> <p>Jacobi迭代的伪代码可以参考如下。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/gAHtQpS.png" width="100%"/> </div> <p>需要注意的是，在某些情况下Jacobi迭代可能会出现不收敛的情况。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/MMyen8l.png" width="100%"/> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/eGUtxaa.png" width="100%"/> </div> <p>要分析不动点迭代法的收敛准则，我们需要引入”误差”的概念。这里”误差”是指当前解\(\boldsymbol{x}^k\)与真实解\(\boldsymbol{x}^*\)之间的差。误差\(\boldsymbol{e}^k\)与残差\(\boldsymbol{r}^k\)之间的关系为</p> \[\boldsymbol{r}^k = \mathbf{b} - \mathbf{A} \boldsymbol{x}^k = \mathbf{A} (\boldsymbol{x}^* - \boldsymbol{x}^k) = \mathbf{A} \boldsymbol{e}^k\] <p>显然，随着迭代的进行误差\(\boldsymbol{e}^k\)具有递推关系</p> \[\boldsymbol{e}^{k+1} \leftarrow (\mathbf{I} - \mathbf{M}^{-1} \mathbf{A}) \boldsymbol{e}^k = \mathbf{M}^{-1} \mathbf{N} \boldsymbol{e}^k\] <p>上式说明，在每一次迭代时误差更新相当于左乘一个矩阵\(\mathbf{T}\)</p> \[\mathbf{T} = \mathbf{I} - \mathbf{M}^{-1} \mathbf{A} = \mathbf{M}^{-1} \mathbf{N}\] <p>只有当矩阵\(\mathbf{T}\)的<a href="https://en.wikipedia.org/wiki/Spectral_radius">谱半径</a>小于1时误差才能保证收敛，否则误差可能会不收敛。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/pr5Jexx.png" width="100%"/> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/heYQMed.png" width="100%"/> </div> <p>在Jacobi迭代中，如果系数矩阵\(\mathbf{A}\)是严格对角占优的则可以保证迭代一定可以收敛。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/N9cgJ7b.png" width="100%"/> </div> <p>如果系数矩阵是非对角占优的，还可以通过松弛的方法使得Jacobi迭代能够收敛。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/UYPMyty.png" width="100%"/> </div> <p>总结来说，Jacobi迭代是一种简单实现且适合并行计算的迭代求解算法。然而，实践经验表明Jacobi迭代通常需要较多的迭代步数才能达到收敛。当系数矩阵接近对角占优时，Jacobi迭代的收敛速度会明显加快。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/DfgxHsF.png" width="100%"/> </div> <h4 id="gauss-seidel迭代">Gauss-Seidel迭代</h4> <p>除了Jacobi迭代，另一种常用的迭代算法是<a href="https://en.wikipedia.org/wiki/Gauss%E2%80%93Seidel_method">Gauss-Seidel迭代</a>。和Jacobi迭代相比，Gauss-Seidel迭代在构造\(\mathbf{M}\)矩阵时使用了系数矩阵的对角元以及下三角部分，此时计算\(\mathbf{M}^{-1}\)可以使用类似与高斯消元法的方式进行处理。由于这样构造的\(\mathbf{M}\)矩阵更接近与系数矩阵\(\mathbf{A}\)，因此Gauss-Seidel迭代通常具有比Jacobi迭代更快的收敛速度和更高的收敛效率。特别值得注意的是，Gauss-Seidel迭代的收敛充分但不是必要条件是系数矩阵\(\mathbf{A}\)为对称半正定矩阵。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/Ve9Rapu.png" width="100%"/> </div> <p>从代码实现的角度来看，Gauss-Seidel迭代和Jacobi迭代的唯一区别在于更新当前解\(\boldsymbol{x}^{k+1}\)时是否使用已经更新过的结果。Gauss-Seidel迭代会利用当前步中已经更新过的计算结果，而Jacobi迭代只会利用上一步的结果。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/ubDv1MH.png" width="100%"/> </div> <p>两种不动点迭代方法的优劣可以总结如下。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/2soxW41.png" width="100%"/> </div> <h3 id="子空间迭代">子空间迭代</h3> <p>除了前面介绍的不动点迭代方法，另一种常用的迭代求解线性系统的方法为子空间迭代法。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/MlERVme.png" width="100%"/> </div> <p>子空间迭代法中最常用的方法是<a href="https://en.wikipedia.org/wiki/Krylov_subspace">Krylov子空间迭代法</a>。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/XbJEgEj.png" width="100%"/> </div> <p>在Krylov子空间的基础上，对于对称半正定系数矩阵的情况我们可以推导<a href="https://en.wikipedia.org/wiki/Conjugate_gradient_method">共轭梯度下降法</a>。其基本思路是将求解矩阵方程\(\mathbf{A} \boldsymbol{x} = \mathbf{b}\)转化为一个最优化问题：</p> \[\min f(\boldsymbol{x}) = \frac{1}{2} \boldsymbol{x}^T \mathbf{A} \boldsymbol{x} - \mathbf{b}^T \boldsymbol{x}\] <p>而优化目标函数的梯度为</p> \[\nabla f = \mathbf{A} \boldsymbol{x} - \mathbf{b} = - \boldsymbol{r}\] <p>共轭梯度法的核心思想是在每次迭代中，将当前的搜索方向与之前的搜索方向正交化(即共轭)，从而在Krylov子空间 \(K_m\)中进行优化。这个子空间是由初始残差和迭代过程中生成的搜索方向构成的。假设我们能够构造出Krylov子空间的一组基\(K_m = \text{span} \{ \boldsymbol{p}_1, ..., \boldsymbol{p}_m \}\)，它们满足共轭条件：</p> \[\boldsymbol{p}_i^T \mathbf{A} \boldsymbol{p}_j = 0\] <p>则在每一步更新\(\boldsymbol{x}_m\)时只需在当前的基上进行移动：</p> \[\boldsymbol{x}_m = \boldsymbol{x}_{m-1} + \alpha_m \boldsymbol{p}_m\] <p>其中步长\(\alpha_m\)可以显式求解。可以证明，只要能够构造出一组共轭基\(K_m = \text{span} \{ \boldsymbol{p}_1, ..., \boldsymbol{p}_m \}\)，共轭梯度法是可以运行下去的。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/XT58RZk.png" width="100%"/> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/nWNDdII.png" width="100%"/> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/Gy79XVV.png" width="100%"/> </div> <p>使用共轭梯度法求解矩阵方程的伪代码可以参考下方ppt。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/J3uIKlW.png" width="100%"/> </div> <p>需要额外说明的是，共轭梯度法的收敛速度取决于系数矩阵\(\mathbf{A}\)的条件数，条件数越接近于1收敛的速度就越快。因此在使用共轭梯度法时往往还会进行一些预处理，以改善条件数并加快收敛速度。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/eSWalkX.png" width="100%"/> </div> <p>当然，除了共轭梯度法之外基于Krylov子空间还有一些其它的线性系统求解方法，它们可以适用于非对称正定矩阵的情况。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/xLLvNUT.png" width="100%"/> </div> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/PMa7SsU.png" width="100%"/> </div> <h2 id="reference">Reference</h2> <ul> <li><a href="https://www.bilibili.com/video/BV1MF4m1V7e3?p=13&amp;vd_source=7a2542c6c909b3ee1fab551277360826">Lecture 13: 线性系统</a></li> </ul>]]></content><author><name></name></author><category term="GAMES001"/><category term="CG"/><category term="Math"/><summary type="html"><![CDATA[求解线性系统的相关技术]]></summary></entry><entry><title type="html">GAMES001课程笔记11-微分方程</title><link href="https://peng00bo00.github.io/blog/2024/GAMES001-NOTES-11/" rel="alternate" type="text/html" title="GAMES001课程笔记11-微分方程"/><published>2024-06-06T00:00:00+00:00</published><updated>2024-06-06T00:00:00+00:00</updated><id>https://peng00bo00.github.io/blog/2024/GAMES001-NOTES-11</id><content type="html" xml:base="https://peng00bo00.github.io/blog/2024/GAMES001-NOTES-11/"><![CDATA[<blockquote class="block-preface"> <p>这个系列是GAMES001-图形学中的数学(<a href="https://games-cn.org/games001/">GAMES 001: Mathematics in Computer Graphics</a>)的同步课程笔记。课程旨在总结归纳图形学学习过程中重要的数学概念、理论和方法，并将以索引的形式在每一章节中穿插讲解该数学专题在图形学中的应用。本课程既可以作为GAMES系列其它课程学习的基础或”手册”，也可以作为站在不一样的视角复习图形学知识的平台。本节主要介绍微分方程的相关知识。</p> </blockquote> <p>微分方程是描述未知函数及其(偏)导数关于自变量之间关系的方程，许多数学和物理问题本质上都可以归结为求解一个微分方程。根据未知函数是否是一元函数，微分方程可以简单分为常微分方程以及偏微分方程两大类。长期以来，人们开发了多种求解微分方程的技术，包括通解法、分离变量法和格林函数法等。而在图形学研究中，我们往往更关注微分方程的数值解法。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/AdLmybR.png" width="100%"/> </div> <h2 id="可分离变量的微分方程">可分离变量的微分方程</h2> <p>分离变量法是一种非常实用的求解常微分方程的方法。它的基本思想是将微分方程中的未知函数及其导数项分离到方程的两边，使每一边只含一个变量。然后，通过同时对两边进行积分来获得方程的通解。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/IshYIAj.png" width="100%"/> </div> <h2 id="一阶线性微分方程">一阶线性微分方程</h2> <p>除了分离变量法外，一些具有特定形式的微分方程可以直接套用公式来进行求解。我们把具有如下标准形式的微分方程称为一阶线性微分方程：</p> \[\frac{\mathrm{d} y}{\mathrm{d} x} + P(x) y = Q(x)\] <p>当\(Q(x) \equiv 0\)时，称方程是齐次的；否则称方程是非齐次的。</p> <h3 id="线性齐次微分方程">线性齐次微分方程</h3> <p>对于线性齐次方程，我们可以通过分离变量法得到微分方程的通解</p> \[y = C e^{- \int P(x) \ \mathrm{d} x}\] <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/zg3KrAl.png" width="100%"/> </div> <h3 id="线性非齐次微分方程">线性非齐次微分方程</h3> <p>对于非齐次的情况则可以通过待定系数法来得到通解</p> \[y = C(x) \cdot e^{- \int P(x) \ \mathrm{d} x}\] \[C(x) = \int Q(x) \cdot e^{- \int P(x) \ \mathrm{d} x} \ \mathrm{d} x + C\] <p>不难发现，线性齐次微分方程的通解实际上就是非齐次微分方程在\(Q(x) = 0\)情况下的特解。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/xpIagbB.png" width="100%"/> </div> <h2 id="一阶非线性微分方程">一阶非线性微分方程</h2> <p>伯努利方程是一类典型的一阶非线性微分方程，它可以通过变量代换法进行求解。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/JcIDLv6.png" width="100%"/> </div> <h2 id="二阶微分方程">二阶微分方程</h2> <p>对于高阶的微分方程，可以考虑通过降阶的方法把它转换为一个已知的低阶微分方程。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/x0767kD.png" width="100%"/> </div> <h3 id="二阶线性微分方程">二阶线性微分方程</h3> <p>在高阶微分方程中，二阶微分方程是一类重要的微分方程。其中，二阶线性微分方程的标准形式为：</p> \[\frac{\mathrm{d}^2 y}{\mathrm{d} x^2} + P(x) \frac{\mathrm{d} y}{\mathrm{d} x} + Q(x) y = f(x)\] <p>根据等式右边\(f(x)\)是否为零，二阶线性微分方程可以划分为齐次和非齐次两种情况。</p> <p>对于二阶线性齐次方程，一个重要的定理是方程的两个线性无关解得线性组合即为方程的通解。这个定理可以拓展到n阶线性齐次方程的情况。类似于一阶线性非齐次方程解的结构，二阶线性非齐次方程的解可以表示为齐次方程的通解加上非齐次方程的特解。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/DnzyY1a.png" width="100%"/> </div> <p>以二阶常系数齐次线性微分方程为例，它的求解过程可以参考如下：</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/1HgfC3E.png" width="100%"/> </div> <p>更高阶的常系数齐次线性微分方程的求解方法与二阶类似，我们可以通过构造特征方程并求根的方式来计算每一项通解。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/2QX1zlL.png" width="100%"/> </div> <h2 id="二阶偏微分方程">二阶偏微分方程</h2> <p>偏微分方程的情况要比常微分方程复杂一些。以二元二阶线性偏微分方程为例，根据系数(函数)的判别式可以将方程分为双曲型、抛物型以及椭圆型偏微分方程三类。这三类偏微分方程都有着大量典型的例子。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/4rxBvlO.png" width="100%"/> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/skVS8sR.png" width="100%"/> </div> <h3 id="波动方程">波动方程</h3> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/ks1iJGL.png" width="100%"/> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/TNGzoAd.png" width="100%"/> </div> <h3 id="拉普拉斯方程">拉普拉斯方程</h3> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/B72Z5qi.png" width="100%"/> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/DmbpoH5.png" width="100%"/> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/ZJW4qpX.png" width="100%"/> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/mkttnYj.png" width="100%"/> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/rem28bE.png" width="100%"/> </div> <h3 id="泊松方程">泊松方程</h3> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/wzReBkl.png" width="100%"/> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/xQmK2KQ.png" width="100%"/> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/rCPfrak.png" width="100%"/> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/CBAbvno.png" width="100%"/> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/W7B2UnA.png" width="100%"/> </div> <h2 id="微分方程的数值解法">微分方程的数值解法</h2> <p>在图形学中我们一般更关心微分方程的数值解法。对于各种各样的微分方程，我们可以通过使用差分来近似微分的方法求解微分方程，这样微分方程就转换为了一个线性系统。关于求解线性系统的相关知识，我们会在下一节进行更深入的介绍。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/mrQaX09.png" width="100%"/> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/lDVPOcQ.png" width="100%"/> </div> <h2 id="reference">Reference</h2> <ul> <li><a href="https://www.bilibili.com/video/BV1MF4m1V7e3?p=12&amp;vd_source=7a2542c6c909b3ee1fab551277360826">Lecture 12: 微分方程</a></li> </ul>]]></content><author><name></name></author><category term="GAMES001"/><category term="CG"/><category term="Math"/><summary type="html"><![CDATA[微分方程的求解方法]]></summary></entry><entry><title type="html">GAMES001课程笔记10-古典微分几何</title><link href="https://peng00bo00.github.io/blog/2024/GAMES001-NOTES-10/" rel="alternate" type="text/html" title="GAMES001课程笔记10-古典微分几何"/><published>2024-06-01T00:00:00+00:00</published><updated>2024-06-01T00:00:00+00:00</updated><id>https://peng00bo00.github.io/blog/2024/GAMES001-NOTES-10</id><content type="html" xml:base="https://peng00bo00.github.io/blog/2024/GAMES001-NOTES-10/"><![CDATA[<blockquote class="block-preface"> <p>这个系列是GAMES001-图形学中的数学(<a href="https://games-cn.org/games001/">GAMES 001: Mathematics in Computer Graphics</a>)的同步课程笔记。课程旨在总结归纳图形学学习过程中重要的数学概念、理论和方法，并将以索引的形式在每一章节中穿插讲解该数学专题在图形学中的应用。本课程既可以作为GAMES系列其它课程学习的基础或”手册”，也可以作为站在不一样的视角复习图形学知识的平台。本节主要介绍古典微分几何的基本知识。</p> </blockquote> <p>微分几何是几何学的一个重要分支，主要通过微积分的方法研究几何对象的性质。根据研究对象的不同，微分几何可以粗略地划分为古典微分几何和现代微分几何。其中，古典微分几何主要研究三维空间中曲线和曲面的各种性质，因此也称为曲线与曲面论；而现代微分几何则更加关注各种流形，在广义相对论中有着重要的研究价值。在计算机图形学中，我们一般只关心三维空间中各种形状的表示方法，因此属于古典微分几何的研究范畴。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/ZbQRq05.png" width="100%"/> </div> <h2 id="参数曲线">参数曲线</h2> <p>在古典微分几何的观点下，三维空间中的曲线可以理解为区间\([a. b]\)到\(\mathbb{R}^3\)的连续映射(向量函数)。当这个映射是连续可微时，我们就可以对它进行求导从而得到曲线上的切向量。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/H5uxk0y.png" width="100%"/> </div> <h3 id="弧长参数">弧长参数</h3> <p>我们定义曲线的弧长为切向量模长沿曲线的积分。实际上可以把弧长作为新的参数构建出新的曲线方程，此时的参数称为曲线的<a href="/2023/07/31/DifferentialGeometry-NOTES-02.html#弧长参数">弧长参数</a>。弧长参数的一个重要性质在于使用弧长作为参数的曲线具有单位长度的切向量，即曲线的切向量场为单位向量场。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/w973KLw.png" width="100%"/> </div> <h3 id="曲率和挠率">曲率和挠率</h3> <p>在弧长参数下，对曲线的单位切向量进行求导可以得到曲线的曲率向量。曲率向量的模称为曲率，而曲率向量的方向则称为曲线的主法向量。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/ElC9sHu.png" width="100%"/> </div> <p>利用曲线的切向量和主法向量可以再通过叉乘得到一个次法向量，这样相当于在曲线上定义了一个右手单位正交坐标系，称为<a href="/2023/07/31/DifferentialGeometry-NOTES-02.html#曲线的曲率和frenet标架">Frenet标架</a>。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/Qbm6is4.png" width="100%"/> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/y7j4Qcj.png" width="100%"/> </div> <p>从运动的角度来看，Frenet标架在曲线上的运动方程称为<a href="/2023/07/31/DifferentialGeometry-NOTES-02.html#曲线的挠率和frenet公式">Frenet公式</a>。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/FHp8vru.png" width="100%"/> </div> <h3 id="曲线论基本定理">曲线论基本定理</h3> <p>曲线论中的一个重要结论是<a href="/2023/07/31/DifferentialGeometry-NOTES-02.html#曲线论基本定理">曲线论基本定理</a>，它指出曲线的形状可以由曲线的曲率和挠率来唯一确定。换句话说，如果空间中两条曲线有着相同的曲率和挠率，则它们之间相差至多一个刚体运动。实际上，如果我们把曲率和挠率看作是关于弧长的函数，则通过求解Frenet方程对应的常微分方程组可以唯一地确定一条曲线。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/KwhMZWN.png" width="100%"/> </div> <h3 id="曲线的近似展开">曲线的近似展开</h3> <p>利用Frenet标架，我们还可以定义曲线在一点上的<a href="/2023/07/31/DifferentialGeometry-NOTES-02.html#曲线的标准展开与近似曲线">近似曲线</a>、<a href="/2023/07/31/DifferentialGeometry-NOTES-02.html#切触">相切</a>以及<a href="/2023/07/31/DifferentialGeometry-NOTES-02.html#曲率圆">曲率圆</a>等概念。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/yf8dzdK.png" width="100%"/> </div> <h2 id="参数曲面">参数曲面</h2> <p>我们把参数曲线的概念进行推广，把\(\mathbb{R}^2\)中一个连通开子集\(D\)到\(\mathbb{R}^3\)的连续映射称为参数曲面。一般地，我们还会要求\(D\)和曲面上的点是一个一一对应，且映射不会出现退化的情况。满足这样性质的曲面称为<a href="/2023/10/18/DifferentialGeometry-NOTES-03.html#正则参数曲面">正则参数曲面</a>。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/n6zyY0H.png" width="100%"/> </div> <p>需要注意的是，正则参数曲面要求曲面上的每一点都可以映射到参数域上，但在很多情况下要获得这样的全局映射是比较困难的。因此，我们可以放宽一下要求，将正则参数曲面拼接的结果称为<a href="/2023/10/18/DifferentialGeometry-NOTES-03.html#正则曲面">正则曲面</a>。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/pcKrKVc.png" width="100%"/> </div> <h3 id="切面与法线">切面与法线</h3> <p>对曲面求偏导就得到了曲面某一点上的两个<a href="/2023/10/18/DifferentialGeometry-NOTES-03.html#切向量">切向量</a>\(\boldsymbol{r}_u\)和\(\boldsymbol{r}_v\)。更进一步，这两个切向量在曲面上定义了一个切空间，称为曲面的<a href="/2023/10/18/DifferentialGeometry-NOTES-03.html#切平面和法线-1">切平面</a>。同时能够证明，对于曲面上任意通过该点的曲线，其切向量一定位于这个切平面上。除此之外，两个切向量的叉乘还定义了曲面在该点处的法线。显然，这个法线也是垂直于切平面的。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/iD7QJFT.png" width="100%"/> </div> <p>曲面上某一点的切向量以及法向量定义了曲面上的一个标架，称为曲面的<a href="/2023/10/18/DifferentialGeometry-NOTES-03.html#自然标架">自然标架</a>。不过不同于曲线的Frenet标架，曲面的自然标架在绝大多数情况下都不是单位正交标架。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/2nocQ1W.png" width="100%"/> </div> <h3 id="曲面的第一基本型">曲面的第一基本型</h3> <p>对曲面上的两个切向量\(\boldsymbol{r}_u\)和\(\boldsymbol{r}_v\)计算内积可以得到三个实数\(E\)、\(F\)、\(G\)，实际上这三个实数给出了曲面上微线元长度的计算公式，它们的矩阵形式\(\begin{pmatrix}E &amp; F \\ F &amp; G\end{pmatrix}\)称为曲面的<a href="/2023/10/18/DifferentialGeometry-NOTES-03.html#第一基本形式">第一基本型</a>。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/OtFfDiD.png" width="100%"/> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/YqNSRGS.png" width="100%"/> </div> <h4 id="曲面上的弧长和面积">曲面上的弧长和面积</h4> <p>曲面的第一基本型给出了切向量作为基底时线元点积的度量。因此<a href="/2023/10/18/DifferentialGeometry-NOTES-03.html#曲面上曲线的弧长和区域面积">曲面上曲线的弧长以及曲面的面积</a>都可以从第一基本型来导出。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/YNzBAaK.png" width="100%"/> </div> <h4 id="保长和保角">保长和保角</h4> <p>第一基本型还可以用来判断两个曲面之间是否存在保长或者保角的变换。实际上，如果两个正则参数曲面之间存在参数变换使得它们具有完全相同的第一基本形式，则它们之间可以建立保长变换。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/TEZGvk5.png" width="100%"/> </div> <p>如果经过参数变换后两个曲面的第一基本形式成正比例，则它们之间可以建立保角变换。可以证明，任意两个正则参数曲面在局部上都可以建立保角映射。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/Gfnj4Km.png" width="100%"/> </div> <h4 id="可展曲面">可展曲面</h4> <p>能够和平面建立保长对应的曲面称为<a href="/2023/10/18/DifferentialGeometry-NOTES-03.html#可展曲面-1">可展曲面</a>。三维空间中的可展曲面一定是柱面、锥面、切线面以及它们的拼接。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/PJ2kcWc.png" width="100%"/> </div> <h3 id="曲面的第二基本型">曲面的第二基本型</h3> <p>曲面的<a href="/2023/11/10/DifferentialGeometry-NOTES-04.html#第二基本形式">第二基本型</a>刻画了曲面在局部偏离切平面弯曲的程度。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/ERue6GI.png" width="100%"/> </div> <p><a href="/2023/11/10/DifferentialGeometry-NOTES-04.html#平面的第二基本形式">平面的第二基本型</a>恒为零，而<a href="/2023/11/10/DifferentialGeometry-NOTES-04.html#球面的第二基本形式">球面的第二基本型</a>是其第一基本型的非零倍数。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/2pQ0IJz.png" width="100%"/> </div> <h4 id="曲面的曲率">曲面的曲率</h4> <p>曲面上的曲率可以通过曲面上的曲线来进行定义。我们把曲线的曲率向量向曲面的法向作投影就得到了曲面在曲线切线方向上的<a href="/2023/11/10/DifferentialGeometry-NOTES-04.html#法曲率">法曲率</a>，它由曲面的第一基本型以及第二基本型共同决定。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/9vy36Af.png" width="100%"/> </div> <p>当曲线的曲率向量方向与曲面的法向重合时，曲面的法曲率与曲线的曲率在数值上是相等的，此时的曲线可以在局部看作是由曲面法向定义的平面与曲面本身的交线，称为法截线。这样，曲面的法曲率可以由法截线的方向向量来进行确定。当法截线的方向绕法向旋转时，曲面上法向量与切线方向之间的关系可以由<a href="/2023/11/10/DifferentialGeometry-NOTES-04.html#euler公式">欧拉公式</a>来进行计算。法曲率取最大值和最小值的方向称为曲面在该点的主方向，对应的法曲率称为主曲率。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/qIusbed.png" width="100%"/> </div> <p>想要更深入的认识曲面的第二基本型需要首先介绍<a href="/2023/11/10/DifferentialGeometry-NOTES-04.html#gauss映射">高斯映射</a>。对于曲面上的法向，我们可以把它看作是具有单位半径的球面上的一个点，这样曲面上的点就通过它的法向和球面建立了一个映射，称为高斯映射。对高斯映射求偏导就得到了<a href="/2023/11/10/DifferentialGeometry-NOTES-04.html#gauss映射">Weingarten映射</a>，它给出了原始曲面上切向量到球面上切向量之间的映射。可以证明Weingarten映射同样是一个线性映射，它的特征值就对应了曲面的主曲率。利用Weingarten映射可以相对容易地推导出欧拉公式。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/e38lL8I.png" width="100%"/> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/5p3Ipy8.png" width="100%"/> </div> <h3 id="曲面论基本定理">曲面论基本定理</h3> <p>类似于曲线论基本定理，<a href="/2023/12/27/DifferentialGeometry-NOTES-05.html#曲面的唯一性定理">曲面论基本定理</a>指出空间中曲面的形状可以由曲面的第一基本型和第二基本型来确定。但需要注意的是，曲面的第一基本型和第二基本型是存在一定的耦合关系的，即<a href="/2023/12/27/DifferentialGeometry-NOTES-05.html#gauss-codazzi方程">Gauss-Codazzi方程</a>。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/KzoutiP.png" width="100%"/> </div> <h2 id="几何连续性">几何连续性</h2> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/FEVHcvO.png" width="100%"/> </div> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/vqThrXT.png" width="100%"/> </div> <h2 id="reference">Reference</h2> <ul> <li><a href="https://www.bilibili.com/video/BV1MF4m1V7e3?p=11&amp;vd_source=7a2542c6c909b3ee1fab551277360826">Lecture 11: 古典微分几何</a></li> </ul>]]></content><author><name></name></author><category term="GAMES001"/><category term="CG"/><category term="Math"/><summary type="html"><![CDATA[古典微分几何的基本知识]]></summary></entry><entry><title type="html">GAMES001课程笔记09-场论初步</title><link href="https://peng00bo00.github.io/blog/2024/GAMES001-NOTES-09/" rel="alternate" type="text/html" title="GAMES001课程笔记09-场论初步"/><published>2024-05-23T00:00:00+00:00</published><updated>2024-05-23T00:00:00+00:00</updated><id>https://peng00bo00.github.io/blog/2024/GAMES001-NOTES-09</id><content type="html" xml:base="https://peng00bo00.github.io/blog/2024/GAMES001-NOTES-09/"><![CDATA[<blockquote class="block-preface"> <p>这个系列是GAMES001-图形学中的数学(<a href="https://games-cn.org/games001/">GAMES 001: Mathematics in Computer Graphics</a>)的同步课程笔记。课程旨在总结归纳图形学学习过程中重要的数学概念、理论和方法，并将以索引的形式在每一章节中穿插讲解该数学专题在图形学中的应用。本课程既可以作为GAMES系列其它课程学习的基础或”手册”，也可以作为站在不一样的视角复习图形学知识的平台。本节主要介绍场论的基本知识。</p> </blockquote> <p>在物理学中我们一般把某个物理量在空间中的分布称为”场”，而在数学中场可以理解为一类特殊的多元函数，其自变量为空间中的位置。本节涉及的场论知识都是在数学意义下的场，因此也可以看作是对矢量分析的入门介绍。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/D0dsdLo.png" width="100%"/> </div> <h2 id="标量场">标量场</h2> <h3 id="梯度">梯度</h3> <p>首先来考虑标量场\(f (x, y, z)\)在给定方向\(\boldsymbol{d} = (d_x, d_y, d_z)\)下的方向导数</p> \[\begin{aligned} \frac{\partial f}{\partial \boldsymbol{d}} &amp;= \lim_{t \to 0} \frac{f(x + t d_x, y + t d_y, z + t d_z) - f(x, y, z)}{t} \\ &amp;= d_x \frac{\partial f}{\partial x} (x, y, z) + d_y \frac{\partial f}{\partial y} (x, y, z) + d_z \frac{\partial f}{\partial z} (x, y, z) \\ &amp;= \boldsymbol{d} \cdot \nabla f \end{aligned}\] <p>即方向导数可以形式化地写成方向向量\(\boldsymbol{d}\)与另一个向量\(\nabla f = \bigg( \frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}, \frac{\partial f}{\partial z} \bigg)\)的内积。我们称\(\nabla f = \text{grad} f = \bigg( \frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}, \frac{\partial f}{\partial z} \bigg)\)为标量场的<strong>梯度(gradient)</strong>。利用方向导数的定义容易证明标量场的梯度方向实际上就是能够使场函数增长最快的方向。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/PwS0IEG.png" width="100%"/> </div> <h2 id="向量场">向量场</h2> <h3 id="通量">通量</h3> <p>除了标量场外，场论中的一个重要内容是研究空间中的向量场。我们定义向量场垂直穿过空间中给定面积的强度为<strong>通量(flux)</strong>，像是流场通过某个截面的流量、光线到达或离开某个面积的大小以及磁感应强度都是向量场的通量。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/enEIdZO.png" width="100%"/> </div> <h3 id="散度">散度</h3> <p>在通量概念的基础上我们可以定义向量场的散度\(\text{div} \boldsymbol{f}\)：</p> \[\text{div} \boldsymbol{f} = \lim_{\Delta V \to 0} \frac{\oint_{\partial V} \boldsymbol{f} \cdot \mathrm{d} A}{\Delta V}\] <p>上式的意义是在空间中取一个体积为\(\Delta V\)的封闭曲面，当曲面趋于无穷小时包含该点的平均通量即为散度。利用\(\nabla\)算子，散度更加常见的形式为</p> \[\text{div} \boldsymbol{f} = \nabla \cdot \boldsymbol{f} = \frac{\partial f_x}{\partial x} + \frac{\partial f_y}{\partial y} + \frac{\partial f_z}{\partial z}\] <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/jOUc12p.png" width="100%"/> </div> <p>散度为0的情况称为<strong>无散(divergence free)</strong>，这说明向量场在该处没有源，对于流场而言它表示流体的体积没有发生改变。有散则与之相反，对于流场而言意味着该点处附近有体积增加或减少。</p> <p>散度进行计算时的一个重要工具是<a href="https://en.wikipedia.org/wiki/Divergence_theorem">高斯散度定理</a>，它指出向量场通过区域\(V\)边界的通量等于散度在整个区域上的积分：</p> \[\oint_{\partial V} (\boldsymbol{f} \cdot \boldsymbol{n}) \ \mathrm{d} A = \iiint_V (\nabla \cdot \boldsymbol{f}) \ \mathrm{d} V\] <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/Icmnx2W.png" width="100%"/> </div> <h3 id="环量">环量</h3> <p>向量场的<strong>环量(circulation)</strong>或者说<strong>涡量(vortex)</strong>定义为向量在闭合曲线切方向上分量的积分，对于流体而言它表示流体沿曲线旋转的强度。</p> \[\Gamma = \oint_L (\boldsymbol{u} \cdot \boldsymbol{\tau}) \ \mathrm{d} l\] <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/aqbq0gC.png" width="100%"/> </div> <h3 id="旋度">旋度</h3> <p>在环量的基础上可以定义向量场的<strong>旋度(curl)</strong>，它在二维平面上的定义为：</p> \[\text{curl} \boldsymbol{f} = \frac{\partial Q}{\partial x} - \frac{\partial P}{\partial y}\] <p>旋度在三维空间中的定义为</p> \[\text{curl} \boldsymbol{f} = \bigg( \frac{\partial R}{\partial y} - \frac{\partial Q}{\partial z}, \frac{\partial P}{\partial z} - \frac{\partial Q}{\partial x}, \frac{\partial Q}{\partial x} - \frac{\partial P}{\partial y} \bigg)\] <p>利用\(\nabla\)算子，旋度可以表示为更简洁的形式</p> \[\text{curl} \boldsymbol{f} = \nabla \times \boldsymbol{f}\] <p>其中，\(\boldsymbol{f} = (P, Q, R)\)。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/9mqpkLi.png" width="100%"/> </div> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/kvl8A9G.png" width="100%"/> </div> <h2 id="矢量微分恒等式">矢量微分恒等式</h2> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/3VTehp0.png" width="100%"/> </div> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/SNwHjAi.png" width="100%"/> </div> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/2YDBgI0.png" width="100%"/> </div> <h2 id="helmholtz分解">Helmholtz分解</h2> <p><a href="https://en.wikipedia.org/wiki/Helmholtz_decomposition">Helmholtz分解</a>是向量场分析中的重要技术，在介绍Helmholtz分解前我们先引入下面几条引理：</p> <ol> <li>有势必无散：若存在向量势函数\(\boldsymbol{\psi}\)使得向量场\(\boldsymbol{A} = \nabla \times \boldsymbol{\psi}\)，则\(\nabla \cdot \boldsymbol{A} = 0\)</li> <li>无散必有势：若向量场满足\(\nabla \cdot \boldsymbol{A} = 0\)，则必存在向量势函数\(\boldsymbol{\psi}\)使得\(\boldsymbol{A} = \nabla \times \boldsymbol{\psi}\)</li> <li>有势必无旋：若存在标量势函数\(\phi\)使得向量场\(\boldsymbol{A} = \nabla \phi\)，则\(\nabla \times \boldsymbol{A} = \boldsymbol{0}\)</li> <li>无旋必有势：若向量场满足\(\nabla \times \boldsymbol{A} = \boldsymbol{0}\)，则必存在标量势函数\(\phi\)使得\(\boldsymbol{A} = \nabla \phi\)</li> </ol> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/nYo1fLo.png" width="100%"/> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/VAcpkPQ.png" width="100%"/> </div> <p>利用上面的引理可以推导出对于无旋无源场\(\boldsymbol{A}\)(\nabla \times \boldsymbol{A} = \boldsymbol{0}, \nabla \cdot \boldsymbol{A} = 0)，一定存在标量势函数\(\phi\)和矢量势函数\(\boldsymbol{\varpsi}\)使得</p> \[\boldsymbol{A} = \nabla \phi = \nabla \times \boldsymbol{\psi}\] <p>更进一步，它们分别满足Laplace方程和双旋度方程</p> \[\nabla \cdot \nabla \phi = 0\] \[\nabla \times \nabla \times \boldsymbol{\psi} = \boldsymbol{0}\] <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/qBFv1Md.png" width="100%"/> </div> <p>最终我们可以得到Helmholtz分解，它指出对于一个一般的向量场\(\boldsymbol{A}\)，它可以分解为无旋场和无源场的叠加：</p> \[\boldsymbol{A} = \boldsymbol{A}_P + \boldsymbol{A}_R = \nabla \phi + \nabla \times \boldsymbol{\psi}\] <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/kZgP9kt.png" width="100%"/> </div> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/em3q60O.png" width="100%"/> </div> <h2 id="外微分">外微分</h2> <p>整体来看矢量分析是比较复杂的，涉及到大量算子之间的运算。借助于外微分这样更先进的数学工具我们可以得到更加简化的表达形式。外微分的一大特点是反对称性，它对应着积分运算中的定向。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/TQQE6qO.png" width="100%"/> </div> <h3 id="外积">外积</h3> <p>外积是外微分中的基本运算符号，它满足如下运算规律：</p> \[\mathrm{d} x \wedge \mathrm{d} x = \mathrm{d} y \wedge \mathrm{d} y = \mathrm{d} z \wedge \mathrm{d} z = 0\] \[\mathrm{d} x \wedge \mathrm{d} y = -\mathrm{d} y \wedge \mathrm{d} x, \ \ \ \mathrm{d} y \wedge \mathrm{d} z = -\mathrm{d} z \wedge \mathrm{d} y, \ \ \ \mathrm{d} z \wedge \mathrm{d} x = -\mathrm{d} x \wedge \mathrm{d} z\] \[( \mathrm{d} x \wedge \mathrm{d} y ) \wedge \mathrm{d} z = \mathrm{d} x \wedge ( \mathrm{d} y \wedge \mathrm{d} z )\] <p>利用外积可以容易地推导出积分的定向。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/ZmCTzbe.png" width="100%"/> </div> <h3 id="外微分形式">外微分形式</h3> <p>接下来我们定义\(\mathbb{R}^3\)中的外微分形式，它们通常是积分中的被积项。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/ZU2vfLj.png" width="100%"/> </div> <p>同时定义微分算子\(\mathrm{d}\)，它将一个k次形式映射为k+1次形式。具体来说，对于0次形式\(f(x, y, z)\)，它的外微分为</p> \[\mathrm{d} f (x, y, z) = \frac{\partial f}{\partial x} \mathrm{d} x + \frac{\partial f}{\partial y} \mathrm{d} y + \frac{\partial f}{\partial z} \mathrm{d} z\] <p>不难发现对于0次形式的外微分，其作用结果与通常意义下的微分是完全一致的。即\(\mathrm{d} \omega_{f}^0 = \omega_{\nabla f}^1\)。</p> <p>对于1次形式则有</p> \[\begin{aligned} \mathrm{d} \omega_{\boldsymbol{A}}^1 &amp;= \mathrm{d} [A_x \mathrm{d} x + A_y \mathrm{d} y + A_z \mathrm{d} z] \\ &amp;= \mathrm{d} A_x \wedge \mathrm{d} x + \mathrm{d} A_y \wedge \mathrm{d} y + \mathrm{d} A_z \wedge \mathrm{d} z \\ &amp;= \omega_{\nabla \times \boldsymbol{A}}^2 \end{aligned}\] <p>即1次形式的外微分产生对应向量场的旋度的2次形式。</p> <p>类似地，对于3次形式则有</p> \[\mathrm{d} \omega_{\boldsymbol{A}}^2 = \mathrm{d} [A_x \mathrm{d} y \wedge \mathrm{d} z + A_y \mathrm{d} z \wedge \mathrm{d} x + A_z \mathrm{d} x \wedge \mathrm{d} y] = \omega_{\nabla \cdot \boldsymbol{A}}^3\] <p>实际上，上面的公式可以总结为<a href="https://en.wikipedia.org/wiki/Poincar%C3%A9_lemma">Poincaré引理</a>：对于任意形式的外微分有\(\mathrm{d}^2 \omega = 0\)。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/btpb39n.png" width="100%"/> </div> <p>利用外微分，我们可以很容易地证明各种矢量分析恒等式。除此之外，微积分中常用的各种积分公式都可以通过外微分的<a href="https://en.wikipedia.org/wiki/Stokes%27_theorem">Stokes公式</a>来进行统一：</p> \[\int_{\partial \Omega} \omega = \int_{\Omega} \mathrm{d} \omega\] <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/KcL1SkC.png" width="100%"/> </div> <h2 id="reference">Reference</h2> <ul> <li><a href="https://www.bilibili.com/video/BV1MF4m1V7e3?p=10&amp;vd_source=7a2542c6c909b3ee1fab551277360826">Lecture 10: 场论初步</a></li> </ul>]]></content><author><name></name></author><category term="GAMES001"/><category term="CG"/><category term="Math"/><summary type="html"><![CDATA[场论的基本知识]]></summary></entry><entry><title type="html">GAMES001课程笔记08-概率论II</title><link href="https://peng00bo00.github.io/blog/2024/GAMES001-NOTES-08/" rel="alternate" type="text/html" title="GAMES001课程笔记08-概率论II"/><published>2024-05-16T00:00:00+00:00</published><updated>2024-05-16T00:00:00+00:00</updated><id>https://peng00bo00.github.io/blog/2024/GAMES001-NOTES-08</id><content type="html" xml:base="https://peng00bo00.github.io/blog/2024/GAMES001-NOTES-08/"><![CDATA[<blockquote class="block-preface"> <p>这个系列是GAMES001-图形学中的数学(<a href="https://games-cn.org/games001/">GAMES 001: Mathematics in Computer Graphics</a>)的同步课程笔记。课程旨在总结归纳图形学学习过程中重要的数学概念、理论和方法，并将以索引的形式在每一章节中穿插讲解该数学专题在图形学中的应用。本课程既可以作为GAMES系列其它课程学习的基础或”手册”，也可以作为站在不一样的视角复习图形学知识的平台。本节主要介绍概率论在图形学中的应用。</p> </blockquote> <h2 id="噪声">噪声</h2> <p>“噪声”的概念源自声学，从物理本质上讲，声音是一种机械波。我们可以将声音与光线的颜色进行类比，声音的频谱对应着不同的颜色。日常生活中常见的噪声大多是一段频率内不同频率的均匀混合，它类似于光谱中白光等于不同色光的混合，因此这样的噪声也称为白噪声。除了白噪声之外，低频强高频弱的噪声称为<a href="https://en.wikipedia.org/wiki/Pink_noise">粉噪声</a>，而低频弱高频强的噪声称为<a href="https://en.wikipedia.org/wiki/Colors_of_noise#Blue_noise">蓝噪声</a>。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/7RFgIPK.png" width="100%"/> </div> <p>广义上的噪声广泛存在于图像、视频等领域中。在图像领域，噪声通常用于指代图像中具有随机扰动的成分，它往往来自于某种随机变量。这些因素可以包括传感器的噪声、信号传输过程中的干扰、环境光线变化等。</p> <p>由于不同的随机变量分布有着不同的频域特征，我们可以借助于图像的频域分析来了解图像上噪声的分布。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/cCc7xEK.png" width="100%"/> </div> <h3 id="图像的量化">图像的量化</h3> <p>对于图像的噪声处理有非常多的应用，这里介绍一下图像的量化。所谓的图像量化是指对于一张给定的RGB图像，我们希望能够找到一张近似图像使得它在每个像素每个通道上只能取0或255两个值，同时还可以尽可能还原原始RGB图像的内容。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/jjQf0xv.png" width="100%"/> </div> <p>首先来考虑一下简化版本的问题：对于一张灰度图，其像素的取值范围在区间[0,1]上，我们希望能够将它转换为一个二值化图像同时尽可能在视觉上接近原图。最简单的二值化方法是使用0.5作为阈值，将图像中所有小于0.5的像素设置为0，而大于0.5的像素设置为1。这种方法非常容易实现，但量化后的图像往往会丢失原始图像中的很多细节。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/EcgKE24.png" width="100%"/> </div> <p>更好的处理方法是有序抖动法。假设显示器的分辨率要高于图像实际的分辨率，那么我们可以使用3×3个显示像素来表示原始图像上的一个像素。具体来说我们设置一个\(M\)矩阵对应填入3×3个显示像素的顺序，若原始像素的亮度值大于等于\(m_{ij} / 9\)，则在3×3个显示像素的对应位置设为1，否则为0。</p> <p>而对于显示分辨率与图像分辨率相同的情况，我们则可以先对原图按照3×3的像素块进行划分，然后在每个3×3的像素块上与\(M\)矩阵进行对比。每个位置上的亮度大于等于\(m_{ij} / 9\)时将该位置的亮度设置为1，否则为0。这样的做法相当于在原图上添加一个以3×3为周期的噪声，然后进行二值化处理。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/b808yMA.png" width="100%"/> </div> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/0KRygvA.png" width="100%"/> </div> <p>从数学的角度来看，上面介绍的图像抖动方法本质是在原始图像上添加一个随机数作为噪声。当添加的随机噪声具有为零的数学期望时，原始像素的期望在添加噪声前后保持不变。因此，当图像的像素点足够多时，图像经过抖动后的期望与原始图像的期望保持不变。这意味着抖动后的图像整体上保持了与原始图像相同的平均亮度水平，从而有助于保持一些原始图像的细节。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/dnVbJjI.png" width="100%"/> </div> <h2 id="蒙特卡洛积分">蒙特卡洛积分</h2> <p>蒙特卡洛积分是图形学中的重要技术，它的一个经典应用是用来计算不规则图形的面积。我们可以在一个固定的矩形区域内随机散落\(n\)个点，然后记落在图形内的点的数量为\(k\)。根据伯努利大数定律，当\(n\)趋于无穷时，\(\frac{k}{n} A_{\text{rectangle}}\)的值会趋于不规则图形面积的真值。</p> <p>更严格的数学解释需要考察积分\(I = \int_a^b f(x) \ \mathrm{d} x\)，蒙特卡洛积分的解法是把积分运算转换为期望运算：</p> \[I = \int_a^b f(x) \ \mathrm{d} x = \int_a^b \frac{f(x)}{p(x)} \cdot p(x) \ \mathrm{d} x = \mathbb{E} \bigg[ \frac{f(x)}{p(x)} \bigg]\] <p>当我们在区间\([a, b]\)上取均匀分布时，蒙特卡洛积分可以表示为</p> \[I \approx \frac{b - a}{n} \sum_{i=1}^n f(X_i)\] <p>对于更一般的分布，蒙特卡洛积分的形式为</p> \[I \approx \frac{1}{n} \sum_{i=1}^n \frac{f(X_i)}{p(X_i)}\] <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/7AvxjN0.png" width="100%"/> </div> <p>容易证明蒙特卡洛积分是一个无偏估计，即当样本数趋于无穷时蒙特卡洛积分会收敛到真值上。而蒙特卡洛积分的方差为</p> \[\begin{aligned} D \bigg[ \frac{1}{n} \sum_{i=1}^n \frac{f(X_i)}{p(X_i)} \bigg] &amp;= \frac{1}{n^2} \sum_{i=1}^n D \bigg[ \frac{f(X_i)}{p(X_i)} \bigg] \\ &amp;= \frac{1}{n^2} \sum_{i=1}^n \int \bigg( \frac{f(x)}{p(x)} - \mathbb{E} \bigg[ \frac{f(X_i)}{p(X_i)} \bigg] \bigg)^2 p(x) \ \mathrm{d} x \\ &amp;\propto \frac{1}{n} \end{aligned}\] <p>上式说明，蒙特卡洛积分的收敛速率是\(\frac{1}{\sqrt{n}}\)。换句话说，如果我们想要把积分的误差缩减到原来的\(\frac{1}{2}\)，我们需要4倍的采样量。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/BqoBPcZ.png" width="100%"/> </div> <p>总体来看，蒙特卡洛积分的收敛是比较慢的。为了提高收敛效率，我们可以采用重要性采样的技术。重要性采样的基本思想是在积分区域内使用一个较好的概率密度函数来选择样本点，从而使得这些样本点更有可能落在函数的主要贡献区域(函数值较大的地方)。特别地，可以证明当采样分布的概率密度函数\(p(x)\)与\(f(x)\)有相同的形状时，蒙特卡洛积分的方差为0。因此，当\(\frac{f(x)}{p(x)}\)的形状越平稳，蒙特卡洛积分的方差越小，也即收敛速度越快。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/StpRlSr.png" width="100%"/> </div> <p>除了重要性采样外，我们还可以使用分层抽样或是低差异序列这样的方法来减少蒙特卡洛积分的方差。这样的方法并没有真正地进行采样，因此也称为拟蒙特卡洛积分。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/Ywq2RJT.png" width="100%"/> </div> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/UpvzJr0.png" width="100%"/> </div> <h2 id="随机数变换">随机数变换</h2> <p>图形学中的一个经典问题是如何在一个单位球面上进行均匀采样。常见的处理方法包括拒绝采样、三角函数变换、利用Gauss分布以及Marsaglia变换等。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/YtkjZjp.png" width="100%"/> </div> <h2 id="reference">Reference</h2> <ul> <li><a href="https://www.bilibili.com/video/BV1MF4m1V7e3?p=9&amp;vd_source=7a2542c6c909b3ee1fab551277360826">Lecture 09: 概率论(二)</a></li> </ul>]]></content><author><name></name></author><category term="GAMES001"/><category term="CG"/><category term="Math"/><summary type="html"><![CDATA[概率论在图形学中的应用]]></summary></entry><entry><title type="html">a post with tabs</title><link href="https://peng00bo00.github.io/blog/2024/tabs/" rel="alternate" type="text/html" title="a post with tabs"/><published>2024-05-01T00:32:13+00:00</published><updated>2024-05-01T00:32:13+00:00</updated><id>https://peng00bo00.github.io/blog/2024/tabs</id><content type="html" xml:base="https://peng00bo00.github.io/blog/2024/tabs/"><![CDATA[<p>This is how a post with <a href="https://github.com/Ovski4/jekyll-tabs">tabs</a> looks like. Note that the tabs could be used for different purposes, not only for code.</p> <h2 id="first-tabs">First tabs</h2> <p>To add tabs, use the following syntax:</p> <div class="language-liquid highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
</pre></td><td class="rouge-code"><pre><span class="cp">{%</span><span class="w"> </span><span class="nt">tabs</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">tab</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="nv">tab-name-1</span><span class="w"> </span><span class="cp">%}</span>

Content 1

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtab</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">tab</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="nv">tab-name-2</span><span class="w"> </span><span class="cp">%}</span>

Content 2

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtab</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtabs</span><span class="w"> </span><span class="cp">%}</span>
</pre></td></tr></tbody></table></code></pre></div></div> <p>With this you can generate visualizations like:</p> <ul id="log" class="tab" data-tab="79737d9f-97ca-4d31-bf70-21690700d805" data-name="log"> <li class="active" id="log-php"> <a href="#">php </a> </li> <li id="log-js"> <a href="#">js </a> </li> <li id="log-ruby"> <a href="#">ruby </a> </li> </ul> <ul class="tab-content" id="79737d9f-97ca-4d31-bf70-21690700d805" data-name="log"> <li class="active"> <div class="language-php highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="nb">var_dump</span><span class="p">(</span><span class="s1">'hello'</span><span class="p">);</span>
</pre></td></tr></tbody></table></code></pre></div></div> </li> <li> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="nx">console</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="dl">"</span><span class="s2">hello</span><span class="dl">"</span><span class="p">);</span>
</pre></td></tr></tbody></table></code></pre></div></div> </li> <li> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="nx">pputs</span> <span class="dl">'</span><span class="s1">hello</span><span class="dl">'</span>
</pre></td></tr></tbody></table></code></pre></div></div> </li> </ul> <h2 id="another-example">Another example</h2> <ul id="data-struct" class="tab" data-tab="9928e89d-e1ab-4b22-92ee-98e5ae817d08" data-name="data-struct"> <li class="active" id="data-struct-yaml"> <a href="#">yaml </a> </li> <li id="data-struct-json"> <a href="#">json </a> </li> </ul> <ul class="tab-content" id="9928e89d-e1ab-4b22-92ee-98e5ae817d08" data-name="data-struct"> <li class="active"> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="rouge-code"><pre><span class="na">hello</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s2">"</span><span class="s">whatsup"</span>
  <span class="pi">-</span> <span class="s2">"</span><span class="s">hi"</span>
</pre></td></tr></tbody></table></code></pre></div></div> </li> <li> <div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="rouge-code"><pre><span class="p">{</span><span class="w">
  </span><span class="nl">"hello"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">"whatsup"</span><span class="p">,</span><span class="w"> </span><span class="s2">"hi"</span><span class="p">]</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></pre></td></tr></tbody></table></code></pre></div></div> </li> </ul> <h2 id="tabs-for-something-else">Tabs for something else</h2> <ul id="something-else" class="tab" data-tab="5d021eb3-b779-4a82-b7ae-bdabaf41ada9" data-name="something-else"> <li class="active" id="something-else-text"> <a href="#">text </a> </li> <li id="something-else-quote"> <a href="#">quote </a> </li> <li id="something-else-list"> <a href="#">list </a> </li> </ul> <ul class="tab-content" id="5d021eb3-b779-4a82-b7ae-bdabaf41ada9" data-name="something-else"> <li class="active"> <p>Regular text</p> </li> <li> <blockquote> <p>A quote</p> </blockquote> </li> <li> <p>Hipster list</p> <ul> <li>brunch</li> <li>fixie</li> <li>raybans</li> <li>messenger bag</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="code"/><summary type="html"><![CDATA[this is what included tabs in a post could look like]]></summary></entry><entry><title type="html">GAMES001课程笔记07-概率论I</title><link href="https://peng00bo00.github.io/blog/2024/GAMES001-NOTES-07/" rel="alternate" type="text/html" title="GAMES001课程笔记07-概率论I"/><published>2024-05-01T00:00:00+00:00</published><updated>2024-05-01T00:00:00+00:00</updated><id>https://peng00bo00.github.io/blog/2024/GAMES001-NOTES-07</id><content type="html" xml:base="https://peng00bo00.github.io/blog/2024/GAMES001-NOTES-07/"><![CDATA[<blockquote class="block-preface"> <p>这个系列是GAMES001-图形学中的数学(<a href="https://games-cn.org/games001/">GAMES 001: Mathematics in Computer Graphics</a>)的同步课程笔记。课程旨在总结归纳图形学学习过程中重要的数学概念、理论和方法，并将以索引的形式在每一章节中穿插讲解该数学专题在图形学中的应用。本课程既可以作为GAMES系列其它课程学习的基础或”手册”，也可以作为站在不一样的视角复习图形学知识的平台。本节主要介绍概率论的基础知识。</p> </blockquote> <h2 id="概率">概率</h2> <h3 id="从频率到概率">从频率到概率</h3> <p>“频率”和”概率”是两个密切相关的概念。我们把\(n\)次试验中事件\(A\)发生的次数除以试验的次数称为”频率”，它是一个落在区间[0， 1]上的实数，记作\(f_n (A)\)。而概率则是定义在样本空间\(S\)上的函数，我们为每一个事件\(A\)赋予一个实数\(P(A)\)，称\(P(A)\)为事件\(A\)的概率。概率\(P(A)\)需要满足非负性、规范性以及可列可加性，同时我们令概率\(P(A)\)等于试验次数\(n\)趋于无穷时频率的极限：</p> \[P(A) = \lim_{n \rightarrow \infty} f_n (A)\] <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/Fre7qk6.png" width="100%"/> </div> <h3 id="古典概型">古典概型</h3> <p>在古典概型中，我们认为样本空间只包含有限多的元素而且每个基本事件发生的可能性相同，因此古典概型也称为等可能概型。根据古典概型的定义，事件\(A\)的概率等于它包含的基本事件的个数除以基本事件的总数。我们在中学数学中学习的排列组合问题就与古典概型密切相关。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/CJVZMz5.png" width="100%"/> </div> <h3 id="条件概率">条件概率</h3> <p>在概率的基础上我们可以定义条件概率。我们定义事件\(A\)已经发生的基础上，事件\(B\)发生的条件概率为</p> \[P(B \vert A) = \frac{P(AB)}{P(A)}\] <p>其中\(P(AB)\)为事件\(A\)和事件\(B\)同时发生的概率。</p> <p>基于条件概率，我们可以进一步推导出全概率公式：</p> \[P(A) = P(A \vert B_1) P(B_1) + P(A \vert B_2) P(B_2) + \cdots + P(A \vert B_n) P(B_n)\] <p>其中，事件\(B_1, \dots B_n\)是对样本空间的一个划分。</p> <p>另一个重要的概率公式是贝叶斯公式：</p> \[P(B_i \vert A) = \frac{P(A \vert B_i) P(B_i)}{\sum_{j=1}^n P(A \vert B_j) P(B_j)}\] <p>贝叶斯公式在数据分析、机器学习等领域都有着重要的应用。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/v2SJuep.png" width="100%"/> </div> <h2 id="随机变量">随机变量</h2> <p>古典概型只适用于基本事件具有相同发生可能性的情况，对于更复杂的问题我们需要引入随机变量的概念。我们定义随机变量\(X\)是定义在样本空间\(S\)上的实值单值函数。根据离散和连续的情况，随机变量可以分为离散型随机变量和连续型随机变量。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/xfX2ug4.png" width="100%"/> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/hsrFlIu.png" width="100%"/> </div> <h3 id="随机变量的数字特征">随机变量的数字特征</h3> <h4 id="数学期望">数学期望</h4> <p>随机变量的数学期望定义为随机变量与其对应的概率相乘再求和：</p> \[\begin{aligned} E(X) &amp;= \sum_k^\infty x_k p_k = \sum_k^\infty P(X \geq k) \\ &amp;= \int_{-\infty}^\infty x f(x) \ \mathrm{d} x \end{aligned}\] <p>数学期望的常用性质可以参考下面的ppt。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/YRD8Fxt.png" width="100%"/> </div> <h4 id="方差">方差</h4> <p>方差描述了随机变量偏离其均值的程度，它的计算公式为</p> \[D(X) = E \{ [X - E(X)]^2 \} = E(X^2) - [E(X)]^2\] <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/hvAVTfE.png" width="100%"/> </div> <h4 id="协方差">协方差</h4> <p>协方差描述了两个随机变量\(X\)和\(Y\)之间的相关性，其计算公式为</p> \[\begin{aligned} \text{cov} (X, Y) &amp;= E \{ [X - E(X)] [Y - E(Y)] \} \\ &amp;= E (XY) - E(X) E(Y) \end{aligned}\] <p>在协方差的基础上，我们还可以定义\(X\)和\(Y\)的相关系数</p> \[\rho_{XY} = \frac{\text{cov} (X, Y)}{\sigma (X) \sigma (Y)}\] <p>协方差的一个性质是相互独立的两个随机变量它们的协方差为0。但需要注意这个命题的逆命题不成立，即协方差为0不能推导出两个随机变量相互独立。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/iic0sJa.png" width="100%"/> </div> <p>协方差的概念还可以推广到多维的情况，此时我们可以通过协方差矩阵来描述每一对随机变量之间的相关性。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/l6JREYg.png" width="100%"/> </div> <h4 id="矩">矩</h4> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/6aOUKYd.png" width="100%"/> </div> <h3 id="泊松分布">泊松分布</h3> <p>泊松分布是一种重要的离散型随机变量分布。令\(X\)表示一段区间内某个事件发生的次数，同时该事件满足如下假定：</p> <ol> <li>一个事件的发生不影响其它事件的发生，即事件相互独立。</li> <li>每个事件发生的概率是相同的。</li> <li>两个事件不能在同一时刻发生。</li> <li>一个区间内事件的概率与区间大小成比例。</li> </ol> <p>设单位区间内事件发生的平均次数为\(\lambda\)，则发生\(k\)次事件的概率为</p> \[P(X = k) = \frac{\lambda^k e^{-\lambda}}{k!}\] <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/FtVsiYk.png" width="100%"/> </div> <h3 id="指数分布">指数分布</h3> <p>指数分布是一种重要的连续型随机变量分布。参数为\(\lambda\)的指数分布的概率密度函数为</p> \[f(x) = \lambda e^{-\lambda x}, \ \ \ x \geq 0\] <p>指数分别的一个重要性质是无记忆性，即</p> \[P \{ X \gt s + t \vert X \gt s \} = P \{ X \gt t \}\] <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/IrytMYF.png" width="100%"/> </div> <h2 id="大数定律和中心极限定理">大数定律和中心极限定理</h2> <h3 id="大数定律">大数定律</h3> <p><a href="https://en.wikipedia.org/wiki/Law_of_large_numbers#Weak_law">弱大数定律</a>指出，相互独立且服从相同分布的随机变量序列\(\{ X_i, X_2, ... \}\)，它们的均值会<strong>依概率</strong>收敛到所属分布的均值上：</p> \[\bar{X} = \frac{1}{n} \sum_{k=1}^n X_k \xrightarrow{P} \mu\] <p>而伯努利大数定律则指出，事件发生的频率依概率收敛于事件的总体概率</p> \[\lim_{n \to\infty} P \bigg\{ \bigg\vert \frac{f_A}{n} - p \bigg\vert \lt \varepsilon \bigg\} = 1\] <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/4NrNTcp.png" width="100%"/> </div> <h3 id="中心极限定理">中心极限定理</h3> <p><a href="https://en.wikipedia.org/wiki/Central_limit_theorem#Classical_CLT">中心极限定理</a>指出，独立同分布的随机变量\(\{ X_i, X_2, ... \}\)的和会趋向于正态分布\(N(n \mu, n \sigma^2)\)：</p> \[\lim_{n \to \infty} \sum_{k=1}^n X_k \rightarrow N(n\mu, n \sigma^2)\] <p>更常见的形式为：</p> \[\lim_{n \to \infty} \frac{1}{n} \sum_{k=1}^n X_k \rightarrow N \bigg( \mu, \frac{\sigma^2}{n} \bigg)\] <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/XhpZ44D.png" width="100%"/> </div> <p>对于非独立同分布的情况，<a href="https://en.wikipedia.org/wiki/Central_limit_theorem#Lyapunov_CLT">Lyapunov定理</a>指出只要随机变量是相互独立的，当满足一定条件时，它们的和仍然能够收敛到正态分布。</p> <p>此外，<a href="https://en.wikipedia.org/wiki/De_Moivre%E2%80%93Laplace_theorem">De Moivre–Laplace定理</a>指出二项分布的极限是标准正态分布。</p> <div align="center"> <img src="https://search.pstatic.net/common?src=https://i.imgur.com/SklIpYW.png" width="100%"/> </div> <h2 id="reference">Reference</h2> <ul> <li><a href="https://www.bilibili.com/video/BV1MF4m1V7e3?p=8&amp;vd_source=7a2542c6c909b3ee1fab551277360826">Lecture 08: 概率论(一)</a></li> </ul>]]></content><author><name></name></author><category term="GAMES001"/><category term="CG"/><category term="Math"/><summary type="html"><![CDATA[概率论基础知识]]></summary></entry><entry><title type="html">a post with typograms</title><link href="https://peng00bo00.github.io/blog/2024/typograms/" rel="alternate" type="text/html" title="a post with typograms"/><published>2024-04-29T23:36:10+00:00</published><updated>2024-04-29T23:36:10+00:00</updated><id>https://peng00bo00.github.io/blog/2024/typograms</id><content type="html" xml:base="https://peng00bo00.github.io/blog/2024/typograms/"><![CDATA[<p>This is an example post with some <a href="https://github.com/google/typograms/">typograms</a> code.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="rouge-code"><pre><span class="p">```</span><span class="nl">typograms
</span><span class="sb">+----+
|    |---&gt; My first diagram!
+----+</span>
<span class="p">```</span>
</pre></td></tr></tbody></table></code></pre></div></div> <p>Which generates:</p> <pre><code class="language-typograms">+----+
|    |---&gt; My first diagram!
+----+
</code></pre> <p>Another example:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
</pre></td><td class="rouge-code"><pre><span class="p">```</span><span class="nl">typograms
</span><span class="sb">.------------------------.
|.----------------------.|
||"https://example.com" ||
|'----------------------'|
| ______________________ |
||                      ||
||   Welcome!           ||
||                      ||
||                      ||
||  .----------------.  ||
||  | username       |  ||
||  '----------------'  ||
||  .----------------.  ||
||  |"*******"       |  ||
||  '----------------'  ||
||                      ||
||  .----------------.  ||
||  |   "Sign-up"    |  ||
||  '----------------'  ||
||                      ||
|+----------------------+|
.------------------------.</span>
<span class="p">```</span>
</pre></td></tr></tbody></table></code></pre></div></div> <p>which generates:</p> <pre><code class="language-typograms">.------------------------.
|.----------------------.|
||"https://example.com" ||
|'----------------------'|
| ______________________ |
||                      ||
||   Welcome!           ||
||                      ||
||                      ||
||  .----------------.  ||
||  | username       |  ||
||  '----------------'  ||
||  .----------------.  ||
||  |"*******"       |  ||
||  '----------------'  ||
||                      ||
||  .----------------.  ||
||  |   "Sign-up"    |  ||
||  '----------------'  ||
||                      ||
|+----------------------+|
.------------------------.
</code></pre> <p>For more examples, check out the <a href="https://google.github.io/typograms/#examples">typograms documentation</a>.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="diagrams"/><summary type="html"><![CDATA[this is what included typograms code could look like]]></summary></entry></feed>